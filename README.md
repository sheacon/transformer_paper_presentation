# Attention Is All You Need

## Paper
A. Vaswani et al., “[Attention Is All You Need](https://arxiv.org/abs/1706.03762)”, (2017).

## Overview
- The problem addressed
- The characterize the approach
- Brief account of how the problem is addressed

## NLP Historical Review
- Other methods, their drawbacks
- How they are still useful/computationally efficient for some tasks
- Lead into how Transformers improves over that

## Architecture Overview
- Formal psuedocode description
- How differs from previous models (LSTM)

## Code Demonstration
- Make a jupyter notebook

## Discussion Questions
- Question 1
- Question 2

## Critical Analysis

## Resource Links
- Paper itself again
- LSTM predecessor
- Subsequent models and improvements
- [Presentation video](https://learning.oreilly.com/videos/natural-language-processing/0636920373605/0636920373605-video329383/)
